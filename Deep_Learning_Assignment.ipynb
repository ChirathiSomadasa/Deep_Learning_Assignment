{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuZwqCZeVOo7dWE7Bv2/Im",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChirathiSomadasa/Deep_Learning_Assignment/blob/Somadasa-N.M.C.U/Deep_Learning_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-rONtf_uC2L",
        "outputId": "520b3b0b-82ae-451a-a5b2-548d3fd54644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip\n",
        "!unzip -q -o \"/content/drive/MyDrive/DL/PlantVillage.zip\" -d \"/content/PlantVillage\""
      ],
      "metadata": {
        "id": "D9GLt18UuFpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvdfYCkFsdTi",
        "outputId": "5e349c48-9b9e-4c52-d208-275cf7b835cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2362 images belonging to 16 classes.\n",
            "Found 590 images belonging to 16 classes.\n",
            "Training samples: 2362, Validation samples: 590\n",
            "Classes: {'Pepper__bell___Bacterial_spot': 0, 'Pepper__bell___healthy': 1, 'Potato___Early_blight': 2, 'Potato___Late_blight': 3, 'Potato___healthy': 4, 'Tomato_Bacterial_spot': 5, 'Tomato_Early_blight': 6, 'Tomato_Late_blight': 7, 'Tomato_Leaf_Mold': 8, 'Tomato_Septoria_leaf_spot': 9, 'Tomato_Spider_mites_Two_spotted_spider_mite': 10, 'Tomato__Target_Spot': 11, 'Tomato__Tomato_YellowLeaf__Curl_Virus': 12, 'Tomato__Tomato_mosaic_virus': 13, 'Tomato_healthy': 14, 'plantvillage': 15}\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 912ms/step - accuracy: 0.3218 - loss: 2.2965 - val_accuracy: 0.6661 - val_loss: 1.0347\n",
            "Epoch 2/5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 758ms/step - accuracy: 0.6623 - loss: 1.0477 - val_accuracy: 0.7610 - val_loss: 0.7599\n",
            "Epoch 3/5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 721ms/step - accuracy: 0.7313 - loss: 0.8193 - val_accuracy: 0.7881 - val_loss: 0.6632\n",
            "Epoch 4/5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 756ms/step - accuracy: 0.7930 - loss: 0.6854 - val_accuracy: 0.7864 - val_loss: 0.6185\n",
            "Epoch 5/5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 713ms/step - accuracy: 0.8055 - loss: 0.5895 - val_accuracy: 0.7932 - val_loss: 0.5868\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 599ms/step - accuracy: 0.8003 - loss: 0.6306\n",
            "Validation Accuracy: 0.81\n"
          ]
        }
      ],
      "source": [
        "# 2️ Fix folder structure if nested\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Check if inner folder exists\n",
        "inner_dir = \"/content/PlantVillage/PlantVillage\"\n",
        "if os.path.exists(inner_dir):\n",
        "    for folder in os.listdir(inner_dir):\n",
        "        src = os.path.join(inner_dir, folder)\n",
        "        dst = os.path.join(\"/content/PlantVillage\", folder)\n",
        "        shutil.move(src, dst)\n",
        "    os.rmdir(inner_dir)\n",
        "\n",
        "\n",
        "# 3️ Create smaller dataset for fast demo\n",
        "\n",
        "original_dir = \"/content/PlantVillage\"\n",
        "fast_dir = \"/content/PlantVillage_FastDemo\"\n",
        "images_per_class = 200  # reduce per class for fast execution\n",
        "\n",
        "if not os.path.exists(fast_dir):\n",
        "    os.makedirs(fast_dir)\n",
        "\n",
        "for class_name in os.listdir(original_dir):\n",
        "    class_path = os.path.join(original_dir, class_name)\n",
        "    if os.path.isdir(class_path):\n",
        "        fast_class_path = os.path.join(fast_dir, class_name)\n",
        "        os.makedirs(fast_class_path, exist_ok=True)\n",
        "\n",
        "        # Copy only image files (ignore subfolders)\n",
        "        img_files = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
        "        img_files = img_files[:images_per_class]  # limit per class\n",
        "\n",
        "        for img in img_files:\n",
        "            src = os.path.join(class_path, img)\n",
        "            dst = os.path.join(fast_class_path, img)\n",
        "            shutil.copy(src, dst)\n",
        "\n",
        "\n",
        "# 4️ Data preprocessing & augmentation\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_data = datagen.flow_from_directory(\n",
        "    fast_dir,\n",
        "    target_size=(128,128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_data = datagen.flow_from_directory(\n",
        "    fast_dir,\n",
        "    target_size=(128,128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {train_data.samples}, Validation samples: {val_data.samples}\")\n",
        "print(\"Classes:\", train_data.class_indices)\n",
        "\n",
        "\n",
        "# 5️ Build MobileNetV2 model\n",
        "\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128,128,3))\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "output = Dense(train_data.num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "\n",
        "\n",
        "# 6️ Train model\n",
        "\n",
        "history = model.fit(train_data, validation_data=val_data, epochs=5, callbacks=[early_stop])\n",
        "\n",
        "\n",
        "# 7️ Evaluate model\n",
        "\n",
        "loss, acc = model.evaluate(val_data)\n",
        "print(f\"Validation Accuracy: {acc:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# 1️ Path to your test image\n",
        "\n",
        "img_path = \"/content/PlantVillage/Pepper__bell___Bacterial_spot/0022d6b7-d47c-4ee2-ae9a-392a53f48647___JR_B.Spot 8964.JPG\"\n",
        "\n",
        "\n",
        "# 2️ Load & preprocess image\n",
        "\n",
        "img = image.load_img(img_path, target_size=(128,128))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = img_array / 255.0  # rescale\n",
        "\n",
        "\n",
        "# 3️ Predict\n",
        "\n",
        "pred = model.predict(img_array)\n",
        "class_index = np.argmax(pred, axis=1)[0]\n",
        "\n",
        "\n",
        "# 4️ Map prediction to class label\n",
        "\n",
        "# Make sure to invert the class_indices dict\n",
        "class_labels = {v: k for k, v in train_data.class_indices.items()}\n",
        "predicted_label = class_labels[class_index]\n",
        "\n",
        "print(f\"Predicted Disease: {predicted_label}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Cl3lXWvzu-S",
        "outputId": "5aeadee5-04d5-43e7-f45d-e296ed106c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "Predicted Disease: Pepper__bell___Bacterial_spot\n"
          ]
        }
      ]
    }
  ]
}